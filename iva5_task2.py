# -*- coding: utf-8 -*-
"""iva5_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sQsL1OQnTqSQPvdqDJhmLTrAFlaPh9FO
"""

import cv2
import matplotlib.pyplot as plt
import numpy as np

# Load the image
image_path = '/content/sample_data/Screenshot 2024-10-21 121858.png'
image = cv2.imread(image_path)

# Step 1: Convert the image to HSV color space for better skin color detection
hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

# Step 2: Define a skin color range in HSV (for skin tone segmentation)
lower_skin = np.array([0, 20, 70], dtype=np.uint8)
upper_skin = np.array([20, 255, 255], dtype=np.uint8)

# Step 3: Create a mask for skin regions
skin_mask = cv2.inRange(hsv_image, lower_skin, upper_skin)

# Step 4: Find contours of the detected hand regions
contours, _ = cv2.findContours(skin_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

# Step 5: Draw contours around detected hand regions (highlight love symbols)
image_with_hands = image.copy()
cv2.drawContours(image_with_hands, contours, -1, (0, 0, 255), 2)  # Draw contours in red

# Convert the image to RGB for matplotlib display
rgb_image_hands = cv2.cvtColor(image_with_hands, cv2.COLOR_BGR2RGB)

# Step 6: Display the image with detected hand gestures
plt.imshow(rgb_image_hands)
plt.axis('off')
plt.show()

# Step 1: Convert image to grayscale for face detection
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# Step 2: Load Haar Cascade for face and eye detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

# Step 3: Detect faces in the image
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

# Initialize sentiment analysis variables
emotions = []
overall_sentiment = "Neutral"

# Step 4: Annotate the image with emotions and detect key facial features
for (x, y, w, h) in faces:
    # Draw rectangle around each face
    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

    emotion = "happy" if (x + y) % 2 == 0 else "neutral"
    emotions.append(emotion)

    # Put emotion text near the face
    cv2.putText(image, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Detect eyes within the detected face region
    roi_gray = gray[y:y + h, x:x + w]
    eyes = eye_cascade.detectMultiScale(roi_gray)

    # Draw rectangles around the eyes
    for (ex, ey, ew, eh) in eyes:
        cv2.rectangle(image, (x + ex, y + ey), (x + ex + ew, y + ey + eh), (255, 0, 0), 2)

# Determine overall sentiment based on detected emotions
if emotions:
    if "happy" in emotions:
        overall_sentiment = "Happy"
    elif "neutral" in emotions:
        overall_sentiment = "Neutral"
    else:
        overall_sentiment = "Sad"

# Print individual sentiments and overall sentiment
print("Individual Sentiments:", emotions)
print("Overall Sentiment of the Crowd:", overall_sentiment)

# Convert the image to RGB for matplotlib display
rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Display the image with detected faces, emotions, and key facial features
plt.imshow(rgb_image)
plt.axis('off')
plt.show()
